{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4442398e-37ad-4d62-b2c9-e183ae2d2a6f",
   "metadata": {},
   "source": [
    "# Intro to Basic Pytorch. CNNs. Image Classification\n",
    "\n",
    "Course: Audio Processing<br>\n",
    "Author: Ostap Viniavskyi<br>\n",
    "Date: Spring 2025<br>\n",
    "\n",
    "## Agenda\n",
    "0. [**Intro**](#Intro)<br>\n",
    "1. [**Introduction to Pytorch**](#Introduction-to-Pytorch)<br>\n",
    "    [1.1 Basic Operations](#Basic-Operations)<br>\n",
    "    [1.2 Intro to Autograd](#Intro-to-Autograd)<br>\n",
    "    [1.3 Solving Non-Linear Least-Squares problem in Pytorch](#Solving-Non-Linear-Least-Squares-problem-in-Pytorch)<br>\n",
    "    \n",
    "2. [**CNN**](#Convolutional-Neural-Networks-(CNN))<br>\n",
    "    [2.1 Convolution operation](#Convolution-operation)<br>\n",
    "    [2.2 CNN building blocks](#CNN-building-blocks)<br>\n",
    "    [2.3 Building CNN in Pytorch](#Building-CNN-in-Pytorch)<br>\n",
    "    \n",
    "3. [**Image Classification**](#Image-Classification)<br>\n",
    "    [3.1 Setting up CNN training pipeline](#Setting-up-CNN-training-pipeline)<br>\n",
    "    [3.2 Evalution and inference of CNN model](#Evalution-and-inference-of-CNN-model)<br>\n",
    "\n",
    "4. [**Conclusions**](#Conclusions)<br>\n",
    "\n",
    "\n",
    "## Intro\n",
    "\n",
    "Deep learning has significantly advanced the field of computer vision, enabling automatic performance of tasks such as image recognition, object detection, and segmentation with high accuracy. Among the most widely used architectures for such tasks are **Convolutional Neural Networks (CNNs)**, which leverage spatial hierarchies in data to learn effective feature representations.  \n",
    "\n",
    "This lecture provides a structured introduction to **PyTorch**, a widely used deep learning framework, and its core functionalities for implementing and training deep learning models. In particular, we will focus on:  \n",
    "\n",
    "- The fundamentals of **PyTorch tensors** and operations.  \n",
    "- **Autograd and backpropagation** for automatic differentiation and gradient computation.  \n",
    "- The architecture and working principles of **Convolutional Neural Networks (CNNs)**.  \n",
    "- **Image classification** using CNNs in PyTorch.  \n",
    "\n",
    "By the end of this session, students will be able to:  \n",
    "1. Understand the basic concepts and operations of PyTorch.  \n",
    "2. Implement **automatic differentiation** and **backpropagation** using PyTorch’s `autograd` module. \n",
    "3. Construct and train a **CNN model** for image classification.  \n",
    "4. Evaluate the performance of the trained model  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e242e29b-2505-48b5-bae7-aa9fe5fdaa5a",
   "metadata": {},
   "source": [
    "### Introduction to Pytorch  \n",
    "\n",
    "PyTorch is an open-source deep learning framework developed by **Meta AI (formerly Facebook AI Research)**. It provides a flexible and efficient platform for building and training neural networks, making it widely used in both research and industry. At its core, PyTorch offers **tensor computation with strong GPU acceleration** and an **automatic differentiation system** (`autograd`), which simplifies gradient-based optimization for deep learning models.  \n",
    "\n",
    "PyTorch excels in **dynamic computation graphs**, allowing users to modify models on the fly, making it highly suitable for applications such as **natural language processing (NLP), computer vision, and reinforcement learning**. The framework provides built-in support for popular deep learning tasks, including **convolutional neural networks (CNNs), recurrent neural networks (RNNs), and transformer-based architectures**.  \n",
    "\n",
    "However, PyTorch does have some limitations. While it is efficient for research and prototyping, its deployment options were traditionally less optimized compared to static graph frameworks like TensorFlow. That said, **TorchScript** and **PyTorch’s integration with ONNX (Open Neural Network Exchange)** have significantly improved its deployment capabilities. Additionally, PyTorch requires **manual optimization for large-scale distributed training**, which can be complex compared to frameworks designed specifically for production environments.  \n",
    "\n",
    "Despite these challenges, PyTorch remains one of the most popular frameworks due to its ease of use, strong community support, and extensive ecosystem, including **TorchVision (for computer vision tasks), TorchText (for NLP), and TorchAudio (for audio processing)**.  \n",
    "\n",
    "\n",
    "**Differences Between PyTorch and NumPy** \n",
    "\n",
    "PyTorch and NumPy both provide powerful tensor computation capabilities, but PyTorch extends NumPy's functionality by **supporting GPU acceleration** and **automatic differentiation**. While NumPy arrays (`ndarray`) are optimized for general numerical computing, PyTorch tensors (`torch.Tensor`) are designed specifically for deep learning, allowing seamless computation on both CPUs and GPUs. Additionally, PyTorch’s `autograd` module enables automatic differentiation, making it easier to compute gradients and train neural networks, whereas NumPy requires explicit differentiation implementations. Despite these differences, PyTorch tensors and NumPy arrays share a similar interface, and conversion between them is straightforward using `.numpy()` and `torch.from_numpy()`.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cff5cb-6b90-47cc-8f8f-78b8f3fe061b",
   "metadata": {},
   "source": [
    "#### Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3990171-e634-4b16-a5be-4733d6689373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchviz import make_dot\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417a5bb-4256-4621-a4c4-ea8fccea028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"CUDA is available. Setting device=\\\"cuda\\\"\")\n",
    "\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"CUDA is not available. Setting device=\\\"cpu\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9aa648-b88f-4597-a091-07357b8a473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create torch tensor from scalar value\n",
    "x = torch.tensor(0.)\n",
    "print(f\"{x.device=}\", f\"{x.shape=}\", f\"{x.dtype=}\")\n",
    "\n",
    "# create 2x2 matrix\n",
    "M = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(f\"{M.device=}\", f\"{M.shape=}\", f\"{M.dtype=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef1d55-f90b-43c6-b99f-a5e8722b8d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move tensor to another device, convert dtype to int64\n",
    "M1 = M.to(device, dtype=torch.int64)\n",
    "print(f\"{M1.device=}\", f\"{M1.dtype=}\")\n",
    "\n",
    "# reshape tensor\n",
    "M2 = M1.reshape(1, 4)\n",
    "print(f\"{M2.shape=}\")\n",
    "\n",
    "M1, M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8284808f-2efd-4a1c-8bd0-c539ed999696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elementwise operations\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(3, 4)\n",
    "\n",
    "C = A + B\n",
    "print(C.shape)\n",
    "\n",
    "try:\n",
    "    C = A + B.T\n",
    "except Exception as e:\n",
    "    print(type(e), e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91395a4f-63cb-4bac-b51b-42fdde2de9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix mulitplication\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(3, 4)\n",
    "\n",
    "D = torch.matmul(A, B.T)\n",
    "print(D.shape)\n",
    "\n",
    "try:\n",
    "    C =  torch.matmul(A, B)\n",
    "except Exception as e:\n",
    "    print(type(e), e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9455972-da3e-4272-a886-5d54eec27cb7",
   "metadata": {},
   "source": [
    "#### Intro to Autograd\n",
    "\n",
    "\n",
    "PyTorch's **Autograd** is an automatic differentiation engine that plays a crucial role in optimizing deep learning models. It allows for efficient gradient computation by tracking operations performed on tensors and dynamically constructing a **computational graph** in the background.\n",
    "\n",
    "**Computational Graph**\n",
    "\n",
    "A **computational graph** is a directed acyclic graph (DAG) where nodes represent mathematical operations, and edges represent the flow of data (tensors). When performing operations on tensors with `requires_grad=True`, PyTorch builds this graph dynamically, recording the sequence of operations for efficient gradient computation during **backpropagation**.\n",
    "\n",
    "**Static vs. Dynamic Computational Graphs**  \n",
    "\n",
    "| Feature           | Static Graph (e.g., TensorFlow v1) | Dynamic Graph (PyTorch) |\n",
    "|------------------|--------------------------------|------------------------|\n",
    "| Graph Definition | Defined before execution      | Built dynamically during execution |\n",
    "| Flexibility      | Less flexible, requires re-compilation for changes | Highly flexible, allowing model modifications on the fly |\n",
    "| Debugging        | More complex, requires specialized tools | Easier, as it integrates with Python’s native debugging tools |\n",
    "| Memory Efficiency | Can be optimized before execution | May use more memory due to dynamic allocation |\n",
    "\n",
    "PyTorch's dynamic computation graph allows for more intuitive and flexible model development, particularly useful for tasks like **variable-length sequences and reinforcement learning**.\n",
    "\n",
    "**Forward vs. Backward Automatic Differentiation**  \n",
    "\n",
    "![](images/forward_autodif.png) <br>\n",
    "\n",
    "1. **Forward Mode Differentiation**  \n",
    "   - Computes derivatives **from inputs to outputs**.  \n",
    "   - Efficient when the number of inputs is **small**, but outputs are large.  \n",
    "   - Less commonly used in deep learning but useful in some scientific computing applications.\n",
    "\n",
    "![](images/backward_autodif.png) <br>\n",
    "2. **Backward Mode Differentiation (Reverse Mode Differentiation)**  \n",
    "   - Computes derivatives **from outputs back to inputs**.  \n",
    "   - Efficient for deep learning, where the number of outputs (loss scalar) is much smaller than the number of parameters.  \n",
    "   - PyTorch primarily uses **backward differentiation** for backpropagation.  \n",
    "\n",
    "PyTorch’s `autograd` enables efficient **backward differentiation** by calling `.backward()` on a scalar loss, computing gradients for all tensors involved in the computation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb10b2c-8b97-44b6-baa2-aaf69f514d64",
   "metadata": {},
   "source": [
    "**Logistic Regression and Reverse Mode Differentiation**  \n",
    "\n",
    "Logistic regression is a binary classification model that predicts the probability of an input belonging to a certain class. Given an input **vector** $\\mathbf{x} \\in \\mathbb{R}^m$ with **m features**, the model computes the probability using the **sigmoid activation function**:  \n",
    "\n",
    "$$\n",
    "y = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "where  \n",
    "\n",
    "$$\n",
    "z = \\mathbf{w}^T \\mathbf{x} + b = \\sum_{i=1}^{m} w_i x_i + b\n",
    "$$\n",
    "\n",
    "- $\\mathbf{w} \\in \\mathbb{R}^m$ is the weight vector  \n",
    "- $b \\in \\mathbb{R}$ is the bias term  \n",
    "- $\\sigma(z)$ is the sigmoid function, which maps $z$ to the range $(0,1)$  \n",
    "\n",
    "\n",
    "The loss function for logistic regression is the **binary cross-entropy loss**, given by:  \n",
    "\n",
    "$$\n",
    "L = - \\left( y_{\\text{true}} \\log y + (1 - y_{\\text{true}}) \\log (1 - y) \\right)\n",
    "$$\n",
    "\n",
    "where $y_{\\text{true}}$ is the ground truth label.\n",
    "\n",
    "**Reverse Mode Differentiation (Backpropagation)**\n",
    "\n",
    "To train the model, we need to compute the **gradients** of the loss $L$ with respect to each **weight value** $w_i$ and bias $b$. Using reverse mode differentiation, we compute derivatives step by step.\n",
    "\n",
    "**Step 1: Compute $\\frac{dL}{dy}$**\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dy} = - \\left( \\frac{y_{\\text{true}}}{y} - \\frac{1 - y_{\\text{true}}}{1 - y} \\right)\n",
    "$$\n",
    "\n",
    "**Step 2: Compute $\\frac{dy}{dz}$ (Derivative of Sigmoid)**\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dz} = \\sigma(z) (1 - \\sigma(z)) = y (1 - y)\n",
    "$$\n",
    "\n",
    "**Step 3: Compute $\\frac{dz}{dw_i}$**  \n",
    "\n",
    "$$\n",
    "\\frac{dz}{dw_i} = x_i\n",
    "$$\n",
    "\n",
    "**Step 4: Compute $\\frac{dL}{dw_i}$ using Chain Rule**\n",
    "\n",
    "By applying the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dw_i} = \\frac{dL}{dy} \\cdot \\frac{dy}{dz} \\cdot \\frac{dz}{dw_i}\n",
    "$$\n",
    "\n",
    "Substituting the values:\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dw_i} = \\left( - \\frac{y_{\\text{true}}}{y} + \\frac{1 - y_{\\text{true}}}{1 - y} \\right) \\cdot y (1 - y) \\cdot x_i\n",
    "$$\n",
    "\n",
    "**Step 5: Interpretation**\n",
    "\n",
    "This derivative tells us how much the loss changes with respect to each input $w_i$. It is used in **gradient-based optimization algorithms**, such as **stochastic gradient descent (SGD)**, to update the model parameters.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b438ca1-54b8-49f3-a80d-2f3648220f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate logistic regression set-up\n",
    "weights_require_grad = True # set to True to avoid error\n",
    "\n",
    "# input data X and label y\n",
    "X = torch.randn(100, 3)\n",
    "y_true = torch.randint(2, size=(100, ))\n",
    "\n",
    "# weights wector w and bias b\n",
    "w = torch.randn(3, requires_grad=weights_require_grad)\n",
    "b = torch.randn(1, requires_grad=weights_require_grad)\n",
    "\n",
    "# compute y_pred\n",
    "y = torch.sigmoid(X @ w + b)\n",
    "\n",
    "# compute binary cross-entropy loss\n",
    "loss = -(1 - y_true) * torch.log(1 - y) - y_true * torch.log(y)\n",
    "loss = loss.sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d0633-637e-486a-a8ba-74e2ec9b9eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dot(loss, params=dict(weights=w, bias=b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d8dcd-885c-4765-9f4a-820f6d646662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print grads\n",
    "print(f\"{w.grad=}\")\n",
    "print(f\"{b.grad=}\")\n",
    "print(f\"{y.grad=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b40ae-c960-41b7-afbf-ea8241b122cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that autodiff results correspond to manually derived values\n",
    "with torch.no_grad():\n",
    "    w_grad = (-y_true / y + (1 - y_true) / (1 - y)) * y * (1 - y) @ X\n",
    "    b_grad = ((-y_true / y + (1 - y_true) / (1 - y)) * y * (1 - y)).sum()\n",
    "    \n",
    "print(f\"{w_grad=}\")\n",
    "print(f\"{b_grad=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab80ea0b-b143-40dd-8898-a1ed57df9dd1",
   "metadata": {},
   "source": [
    "<a id=\"Solving-Non-Linear-Least-Squares-problem-in-Pytorch\"></a>\n",
    "#### Solving Non-Linear Least-Squares problem in Pytorch\n",
    "\n",
    "![](images/logistic_curve.png) <br>\n",
    "\n",
    "**Logistic Curve Fitting (Non-Linear Least Squares)**  \n",
    "\n",
    "The logistic function is defined as:  \n",
    "\n",
    "$$\n",
    "f(x; L, k, x_0) = \\frac{L}{1 + e^{-k(x - x_0)}}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ L $ is the maximum value (asymptote),  \n",
    "- $ k $ is the growth rate,  \n",
    "- $ x_0 $ is the midpoint (where $ f(x) = L/2 $),  \n",
    "- $ x $ is the input variable.  \n",
    "\n",
    "**Least Squares Loss Function** \n",
    "\n",
    "To fit a logistic curve to given data points $ (x_i, y_i) $, we minimize the **mean squared error (MSE)**:  \n",
    "\n",
    "$$\n",
    "J(L, k, x_0) = \\frac{1}{N} \\sum_{i=1}^{N} \\left( f(x_i; L, k, x_0) - y_i \\right)^2\n",
    "$$\n",
    "\n",
    "where $ N $ is the number of data points.\n",
    "\n",
    "To update parameters using **vanilla gradient descent**, we compute the partial derivatives of the loss function with respect to each parameter.\n",
    "\n",
    "**Gradient Descent Update Rule**\n",
    "\n",
    "Using vanilla gradient descent, we update the parameters as follows:\n",
    "\n",
    "$$\n",
    "L^{(t+1)} = L^{(t)} - \\alpha \\frac{\\partial J}{\\partial L}\n",
    "$$\n",
    "\n",
    "$$\n",
    "k^{(t+1)} = k^{(t)} - \\alpha \\frac{\\partial J}{\\partial k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_0^{(t+1)} = x_0^{(t)} - \\alpha \\frac{\\partial J}{\\partial x_0}\n",
    "$$\n",
    "\n",
    "where $ \\alpha $ is the learning rate and $ t $ is the iteration index.\n",
    "\n",
    "**Iterative Procedure**\n",
    "\n",
    "1. Initialize $ L, k, x_0 $ with random values.  \n",
    "2. Compute the loss $ J(L, k, x_0) $.  \n",
    "3. Compute the gradients $ \\frac{\\partial J}{\\partial L}, \\frac{\\partial J}{\\partial k}, \\frac{\\partial J}{\\partial x_0} $.  \n",
    "4. Update parameters using the gradient descent update rules.  \n",
    "5. Repeat until convergence (e.g., when the loss stops decreasing significantly). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d64067-dee7-4d58-8b94-58f40719ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data from a logistic curve\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def logistic(x, L, k, x0):\n",
    "    return L / (1 + torch.exp(-k * (x - x0)))\n",
    "\n",
    "# True parameters\n",
    "L_true = 10.0\n",
    "k_true = 1.5\n",
    "x0_true = 5.0\n",
    "\n",
    "# Generate x values\n",
    "x = torch.linspace(0, 10, steps=100)\n",
    "y_true = logistic(x, L_true, k_true, x0_true)\n",
    "\n",
    "# Add noise\n",
    "noise_std = 2.\n",
    "y_noisy = y_true + noise_std * torch.randn_like(y_true)\n",
    "\n",
    "plt.scatter(x.numpy(), y_noisy.numpy(), label='Noisy Data', alpha=0.6)\n",
    "plt.plot(x.numpy(), y_true.numpy(), label='True Curve', linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f493408-14b9-4892-8f46-4b787237354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters randomly\n",
    "L = torch.tensor(5.0, requires_grad=True)\n",
    "k = torch.tensor(1.0, requires_grad=True)\n",
    "x0 = torch.tensor(-2.0, requires_grad=True)\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.05\n",
    "num_epochs = 700\n",
    "\n",
    "# Gradient descent\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = logistic(x, L, k, x0)\n",
    "    loss = torch.mean((y_pred - y_noisy) ** 2)  # Mean Squared Error\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Manual update\n",
    "    with torch.no_grad():\n",
    "        L -= lr * L.grad\n",
    "        k -= lr * k.grad\n",
    "        x0 -= lr * x0.grad\n",
    "    \n",
    "    # Zero gradients\n",
    "    L.grad.zero_()\n",
    "    k.grad.zero_()\n",
    "    x0.grad.zero_()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.scatter(x.numpy(), y_noisy.numpy(), label='Noisy Data', alpha=0.6)\n",
    "plt.plot(x.numpy(), logistic(x, L, k, x0).detach().numpy(), label='Fitted Curve', color='red')\n",
    "plt.plot(x.numpy(), y_true.numpy(), label='True Curve', linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print learned parameters\n",
    "print(f\"Learned parameters: L={L.item():.2f}, k={k.item():.2f}, x0={x0.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc30f49d-57f8-4e43-ab5b-032fda3385f3",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b514a7-6765-4be2-94ff-03b9b1910779",
   "metadata": {},
   "source": [
    "#### Convolution operation\n",
    "\n",
    "![](images/conv_filter.gif) <br>\n",
    "\n",
    "In a 2D convolution operation, a small matrix called a **kernel** or **filter** slides over a larger input matrix (typically an image) to produce an output feature map. Mathematically, for an input $ X $ of size $ H \\times W $ and a kernel $ K $ of size $ k_h \\times k_w $, the convolution at position $ (i, j) $ is computed as:  \n",
    "\n",
    "$$\n",
    "Y(i, j) = \\sum_{m=0}^{k_h-1} \\sum_{n=0}^{k_w-1} X(i+m, j+n) \\cdot K(k_h - 1 - m, k_w - 1 - n)\n",
    "$$\n",
    "\n",
    "This operation captures **spatial patterns** by detecting local features such as edges, textures, and shapes. **Padding** can be applied to preserve spatial dimensions, while **strides** determine the step size of the kernel's movement. Multiple filters in a convolutional layer allow the extraction of diverse features, forming the foundation of deep learning architectures for image processing.\n",
    "\n",
    "**EXAMPLE: 2D Laplacian of Gaussian (LoG)**  \n",
    "\n",
    "The **Laplacian of Gaussian (LoG)** is a second-order edge detection operator that enhances regions of rapid intensity change. It is computed by first applying a **Gaussian blur** to smooth the image and then taking the **Laplacian**, which measures the second derivative of intensity. Mathematically, the **LoG filter** is defined as:\n",
    "\n",
    "$$\n",
    "\\text{LoG}(x, y) = \\nabla^2 (G(x, y) * I(x, y))\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ G(x, y) $ is a Gaussian function that reduces noise,\n",
    "- $ I(x, y) $ is the input image,\n",
    "- $ \\nabla^2 $ is the **Laplacian operator**, which detects regions of high curvature.\n",
    "\n",
    "**Applications in Blob Detection**\n",
    "\n",
    "LoG is widely used in **blob detection**, where blobs are defined as regions that differ significantly in intensity from their surroundings. The **zero-crossings** in the LoG output indicate blob-like structures, making it effective for tasks such as:\n",
    "- **Keypoint detection** in image processing (e.g., in **SIFT** feature extraction).\n",
    "- **Medical imaging**, where it highlights circular or irregular structures (e.g., cell or tumor detection).\n",
    "- **Astronomy**, for identifying celestial objects like stars and galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526f0dd7-33b8-4c77-ba52-ff748d6d725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d69359a-86b7-414d-ac1d-a69b74e1a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image in grayscale\n",
    "image = cv2.imread('images/sunplowers.jpeg', cv2.IMREAD_GRAYSCALE)\n",
    "print(image.shape)\n",
    "\n",
    "# Convert image to float32 and normalize to 0-1 range\n",
    "image = image.astype(np.float32) / 255.\n",
    "H, W = image.shape\n",
    "\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54793425-b3ef-4e25-9a4e-f87ebbea8651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot image as a 2d function\n",
    "# xx, yy = np.meshgrid(np.arange(W), np.arange(H))\n",
    "# fig = plt.figure(figsize=(14, 6))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.plot_surface(xx, yy, image, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef64a17-d64e-43c3-bb07-90009e9a2027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Laplacian of Gaussian (LoG) kernel\n",
    "def create_log_kernel(sigma):\n",
    "    kernel_size = int(2 * np.ceil(3 * sigma) + 1)\n",
    "    impulse_image = np.zeros((kernel_size, kernel_size))\n",
    "    impulse_image[kernel_size // 2, kernel_size // 2] = 1  # Center pixel is 1\n",
    "\n",
    "    return gaussian_laplace(impulse_image, sigma=sigma)\n",
    "\n",
    "\n",
    "def visualize_log_kernel(log_kernel):\n",
    "    kernel_size = log_kernel.shape[0]\n",
    "    ax = np.linspace(-(kernel_size // 2), kernel_size // 2, kernel_size)\n",
    "    xx, yy = np.meshgrid(ax, ax)\n",
    "    \n",
    "    fig = plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    ax = fig.add_subplot(121, projection='3d')\n",
    "    ax.plot_surface(xx, yy, log_kernel, cmap='coolwarm')\n",
    "    \n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.imshow(log_kernel, cmap='coolwarm')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "log_kernel_s5 = create_log_kernel(sigma=5).astype(np.float32)\n",
    "log_kernel_s10 = create_log_kernel(sigma=10).astype(np.float32)\n",
    "visualize_log_kernel(log_kernel_s10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c397b9-c3d0-47eb-bf1a-0fa2e049c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def convolve_gray(image: np.ndarray, kernel: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform convolution on a grayscale image using the given kernel.\n",
    "\n",
    "    :param image: 2D NumPy array representing the grayscale image\n",
    "    :param kernel: 2D NumPy array representing the convolution filter\n",
    "    :return: 2D NumPy array of the convolved image\n",
    "    \"\"\"\n",
    "    # Get dimensions\n",
    "    img_h, img_w = image.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    \n",
    "    # Compute padding size\n",
    "    pad_h = kernel_h // 2\n",
    "    pad_w = kernel_w // 2\n",
    "    \n",
    "    # Pad the image with zeros\n",
    "    padded_image = np.pad(image, ((pad_h, pad_h), (pad_w, pad_w)), mode='constant', constant_values=0)\n",
    "    \n",
    "    # Output image\n",
    "    output = np.zeros((img_h, img_w), dtype=np.float32)\n",
    "    \n",
    "    # Flip the kernel for convolution\n",
    "    kernel = np.flipud(np.fliplr(kernel))\n",
    "    \n",
    "    # Perform convolution\n",
    "    for i in range(img_h):\n",
    "        for j in range(img_w):\n",
    "            patch = padded_image[i:i+kernel_h, j:j+kernel_w]\n",
    "            output[i, j] = np.sum(patch * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "log5_image = convolve_gray(image, log_kernel_s5)\n",
    "log10_image = convolve_gray(image, log_kernel_s10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c686eb-6e38-4f53-a8ba-61d6afdb945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "ax = fig.add_subplot(121,)\n",
    "ax.imshow(log5_image, cmap='gray')\n",
    "ax.title.set_text(\"Convolution of input image with LoG(sigma=5)\")\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.imshow(log10_image, cmap='gray')\n",
    "ax.title.set_text(\"Convolution of input image with LoG(sigma=10)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a394fa3-1ee1-4354-be5c-e42841b806cd",
   "metadata": {},
   "source": [
    "*Additional Resourses*: <br>\n",
    "[3Blue1Brown video - Intuitive explanation of convolution operation](https://www.youtube.com/watch?v=KuXjwB4LzSA&ab_channel=3Blue1Brown) <br>\n",
    "[Stanford Noted - Great convolution output size explanation and other CNN-related stuff](https://cs231n.github.io/convolutional-networks/) <br>\n",
    "[Medium article - Efficient convolution implementation strategies](https://medium.com/@sundarramanp2000/different-implementations-of-the-ubiquitous-convolution-6a9269dbe77f) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ffe85-1e54-4d15-806a-a44b17272b28",
   "metadata": {},
   "source": [
    "#### CNN building blocks\n",
    "\n",
    "**Multichannel Convolution**\n",
    "\n",
    "In deep learning, **multichannel convolution** extends the standard 2D convolution operation to handle inputs with multiple channels, such as RGB images or feature maps in deeper layers of a neural network. Unlike single-channel convolution, where a **2D kernel** slides over a single input, in multichannel convolution, a separate kernel is applied to **each input channel**, and the results are summed to produce a **single output value** per spatial location.  \n",
    "\n",
    "For an input tensor of size $H \\times W \\times C_{\\text{in}}$ (height, width, and number of channels), a convolutional layer with $C_{\\text{out}}$ filters applies a **kernel of size** $k_h \\times k_w \\times C_{\\text{in}}$ per filter. Each filter produces a single output channel, resulting in an output tensor of size $H' \\times W' \\times C_{\\text{out}}$, where $H'$ and $W'$ depend on padding and stride.  \n",
    "\n",
    "*Example*:  \n",
    "- **Input:** $32 \\times 32 \\times 3$ (RGB image)  \n",
    "- **Kernel:** $3 \\times 3 \\times 3$  \n",
    "- **Output (with 16 filters):** $30 \\times 30 \\times 16$ (if no padding, stride = 1)  \n",
    "\n",
    "In deeper layers, multichannel convolution enables networks to learn complex hierarchical features by combining multiple filters. This is essential for tasks like **image classification, object detection, and segmentation**.\n",
    "\n",
    "**MaxPooling 2D (MaxPool2D)** \n",
    "\n",
    "**MaxPooling 2D** (MaxPool2D) is a downsampling operation commonly used in convolutional neural networks (CNNs) to reduce spatial dimensions while retaining important features. It operates by sliding a **fixed-size window** (e.g., $2 \\times 2$ or $3 \\times 3$) over the input feature map and selecting the **maximum value** within each window. This process helps to reduce computation, increase translation invariance, and improve robustness to small spatial variations.  \n",
    "\n",
    "For an input feature map of size $H \\times W \\times C$, applying a max-pooling operation with a window of size $k_h \\times k_w$ and stride $s$ results in an output feature map of size:\n",
    "\n",
    "$$\n",
    "H' = \\frac{H - k_h}{s} + 1, \\quad W' = \\frac{W - k_w}{s} + 1\n",
    "$$\n",
    "\n",
    "*Example*:\n",
    "- **Input:** $32 \\times 32 \\times 64$ feature map  \n",
    "- **MaxPool:** $2 \\times 2$ window, stride = 2  \n",
    "- **Output:** $16 \\times 16 \\times 64$  \n",
    "\n",
    "Unlike convolution, max-pooling does not have learnable parameters—it simply selects the strongest activations. This makes it an effective tool for reducing spatial size while preserving dominant features, helping CNNs generalize better for tasks like **image classification and object detection**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50143546-2682-4643-bab6-6297f79ffbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf36231-b357-457a-960b-06b538ca1571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert image and kernel to torch\n",
    "image_t_cpu = torch.tensor(image, dtype=torch.float32, device=\"cpu\")\n",
    "log_kernel_s5_t_cpu = torch.tensor(log_kernel_s5, dtype=torch.float32, device=\"cpu\")\n",
    "log_kernel_s10_t_cpu = torch.tensor(log_kernel_s10, dtype=torch.float32, device=\"cpu\")\n",
    "\n",
    "# unsqueeze dimensions in the tensor\n",
    "# image [H, W] -> [B=1, C=1, H, W]\n",
    "image_t_cpu = image_t_cpu.unsqueeze(0).unsqueeze(0)\n",
    "print(f\"{image_t_cpu.shape=}\")\n",
    "\n",
    "# kernel [KH, KW] -> [N=1, M=1, KH, KW]\n",
    "log_kernel_s5_t_cpu = log_kernel_s5_t_cpu.unsqueeze(0).unsqueeze(0)\n",
    "print(f\"{log_kernel_s5_t_cpu.shape=}\")\n",
    "\n",
    "log_kernel_s10_t_cpu = log_kernel_s10_t_cpu.unsqueeze(0).unsqueeze(0)\n",
    "print(f\"{log_kernel_s10_t_cpu.shape=}\")\n",
    "\n",
    "# create GPU version of tensors\n",
    "image_t_device = image_t_cpu.to(device)\n",
    "log_kernel_s5_t_device = log_kernel_s5_t_cpu.to(device)\n",
    "log_kernel_s10_t_device = log_kernel_s10_t_cpu.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b8930f-ef2e-4e3d-acc6-5b525c3ef641",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "log5_image_t_cpu = F.conv2d(image_t_cpu, log_kernel_s5_t_cpu, padding=log_kernel_s5_t_device.shape[-1]//2)\n",
    "log10_image_t_cpu = F.conv2d(image_t_cpu, log_kernel_s10_t_cpu, padding=log_kernel_s10_t_device.shape[-1]//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1391da-5188-4058-9579-184d211f56d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "log5_image_t_device = F.conv2d(image_t_device, log_kernel_s5_t_device, padding=log_kernel_s5_t_device.shape[-1]//2)\n",
    "log10_image_t_device = F.conv2d(image_t_device, log_kernel_s10_t_device, padding=log_kernel_s10_t_device.shape[-1]//2)\n",
    "torch.cuda.synchronize(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d4808-636f-4169-a98c-be243d9dfe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{log5_image_t_device.shape=}\")\n",
    "print(f\"{log10_image_t_device.shape=}\")\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.imshow(log5_image_t_device.cpu()[0, 0], cmap='gray')\n",
    "ax.title.set_text(\"Convolution of input image with LoG(sigma=5)\")\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.imshow(log10_image_t_device.cpu()[0, 0], cmap='gray')\n",
    "ax.title.set_text(\"Convolution of input image with LoG(sigma=10)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c81f9f9-7050-4471-a937-0440505a7c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool_out = F.max_pool2d(log5_image_t_device, kernel_size=8, stride=8, )\n",
    "\n",
    "print(f\"{max_pool_out.shape=}\")\n",
    "plt.imshow(max_pool_out.cpu()[0, 0], cmap='gray')\n",
    "plt.title(\"Max pool with KS=8, stride=8 of LoG of input image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71797b13-005d-4b75-aa63-dcf581f20b1c",
   "metadata": {},
   "source": [
    "#### Building CNN in Pytorch\n",
    "\n",
    "**What CNNs Learn Internally and the Concept of Receptive Field** \n",
    "\n",
    "![](images/feat_by_layers.webp) <br>\n",
    "\n",
    "In a Convolutional Neural Network (CNN), the filters in different layers learn to detect features of increasing complexity. In the **early layers**, filters typically learn to detect **low-level patterns** such as edges, textures, and simple shapes. As the network goes deeper, these filters begin to recognize **mid-level patterns** like corners, contours, and more complex textures. In the **final layers**, the network learns **high-level semantic features**, such as object parts or entire objects, which are crucial for classification and detection tasks.  \n",
    "\n",
    "A key concept in CNNs is the **receptive field**, which refers to the region of the input image that influences a particular neuron in a feature map. In the **earlier layers**, the receptive field is small, meaning each neuron captures only local patterns. As we move deeper into the network, stacking convolution and pooling layers increases the **effective receptive field**, allowing neurons to capture larger spatial relationships. This hierarchical feature extraction enables CNNs to recognize objects at different scales and positions, making them highly effective for visual tasks. \n",
    "\n",
    "\n",
    "**Translation Equivariance in Convolution**\n",
    "\n",
    "![](images/equivariance.webp) <br>\n",
    "\n",
    "One of the fundamental properties of convolution is **translation equivariance**. This means that if an object in the input image shifts spatially, the output feature map shifts accordingly but remains otherwise unchanged. Mathematically, if \\( f(X) \\) represents a convolution operation on an input image \\( X \\), and \\( T \\) is a translation operator, then:\n",
    "\n",
    "$$\n",
    "f(T(X)) = T(f(X))\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec51b4cb-62c5-40f3-8b8f-b1329b906ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from torchview import draw_graph\n",
    "\n",
    "import graphviz\n",
    "graphviz.set_jupyter_format('png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c79775d-cb9b-43fe-a585-d6d95a6c2a9b",
   "metadata": {},
   "source": [
    "**AlexNet**, introduced by **Krizhevsky et al. (2012)**, was a groundbreaking deep learning model that won the **ImageNet Large Scale Visual Recognition Challenge (ILSVRC-2012)** with a **top-5 error rate of 15.3%**, significantly outperforming traditional computer vision methods. It was one of the first deep **Convolutional Neural Networks (CNNs)** to demonstrate the power of deep learning in large-scale image classification.  \n",
    "\n",
    "AlexNet consists of **eight layers**: five **convolutional layers** followed by three **fully connected layers**. The architecture employs **ReLU activations** instead of traditional sigmoid/tanh functions, enabling faster training. It also introduces **dropout** in the fully connected layers to reduce overfitting. To efficiently train on large datasets, AlexNet used **GPU acceleration** with two parallel NVIDIA GTX 580 GPUs. Additionally, **overlapping max-pooling** was used to enhance translation invariance, and **data augmentation** (such as random cropping and flipping) was applied to improve generalization.  \n",
    "\n",
    "AlexNet's success marked the beginning of the **deep learning revolution**, inspiring later architectures such as **VGG, ResNet, and DenseNet**. Despite being relatively simple by modern standards, it remains a foundational model in computer vision.\n",
    "\n",
    "![](images/alexnet.png) <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53343393-9b15-4c0b-95c3-a271a62fb095",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(256, 384, kernel_size=3, padding=1)        \n",
    "        self.conv4 = nn.Conv2d(384, 384, kernel_size=3, padding=1)        \n",
    "        self.conv5 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Pooling layers\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, num_classes)\n",
    "\n",
    "        # Dropout layers\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.relu(self.conv5(x))\n",
    "        x = self.pool5(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)  # Flatten from (B, 256, 6, 6) to (B, 256*6*6)\n",
    "\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "model = AlexNet(num_classes=10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211482f2-5575-49c1-b069-76aff376f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_model_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{num_model_params=}\")\n",
    "\n",
    "model_graph = draw_graph(model, input_size=(1, 3, 227, 227), expand_nested=True, device=\"meta\")\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17133a1-1d9b-4211-94ac-215b5031dbe6",
   "metadata": {},
   "source": [
    "### Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e90d2-a561-4bd7-aa7e-e8265aa8b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2251f0c-d5c4-408f-87ec-a9f350a549eb",
   "metadata": {},
   "source": [
    "#### Setting up CNN training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e7db62-b314-4f21-acc7-f6ae3e370482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(227),  # Resize for AlexNet\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "# Define CIFAR-10 class names\n",
    "classes = ('airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Get a batch of training data\n",
    "data_iter = iter(trainloader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Denormalize images for correct visualization\n",
    "def denormalize(img):\n",
    "    img = img * 0.5 + 0.5  # Reverse normalization\n",
    "    return img.clamp(0, 1)  # Ensure values are in valid range\n",
    "\n",
    "# Plot images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5))  # 2 rows, 5 columns\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    img = denormalize(images[i]) \n",
    "    img = img.permute(1, 2, 0)\n",
    "    \n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(classes[labels[i].item()])\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f718bd2f-c985-4a22-84db-34e831b99859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "model = AlexNet(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2db9c9-3114-434c-ac5e-9d0dae4e9afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "    \n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(trainloader):.4f}\")\n",
    "    \n",
    "    # Evaluate model after each epoch\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy after epoch {epoch+1}: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d29b2-81e5-436f-b4b5-ce140638131e",
   "metadata": {},
   "source": [
    "#### Evalution and inference of CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f39a18-b88a-4411-9ce8-40ce494406fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test data\n",
    "data_iter = iter(testloader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Move images to the same device as the model\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "# Make predictions\n",
    "model.eval()  \n",
    "with torch.no_grad():\n",
    "    outputs = model(images) \n",
    "    _, predicted = torch.max(outputs, 1) \n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5))  # 2 rows, 5 columns\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    img = denormalize(images[i].cpu()) \n",
    "    img = img.permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "\n",
    "    true_label = classes[labels[i].item()]\n",
    "    pred_label = classes[predicted[i].item()]\n",
    "    \n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"True: {true_label}\\nPred: {pred_label}\", fontsize=10, color=\"green\" if true_label == pred_label else \"red\")\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93605edb-4c8f-4e24-9ac8-3a1fc408bfef",
   "metadata": {},
   "source": [
    "**How to improve the model**: \n",
    "- investigtate if the model overfits -- prevent overfitting by [regularization](https://medium.com/analytics-vidhya/understanding-regularization-with-pytorch-26a838d94058)\n",
    "- add [BatchNorm operation](https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739/)\n",
    "- pretrained model initialization -- explore [Transfer Learning](https://towardsdatascience.com/transfer-learning-for-beginner-9b59490d1b9d/)\n",
    "- add [augmentations](https://towardsdatascience.com/complete-guide-to-data-augmentation-for-computer-vision-1abe4063ad07/)\n",
    "- explore other architectures -- [ResNet](https://towardsdatascience.com/resnets-why-do-they-perform-better-than-classic-convnets-conceptual-analysis-6a9c82e06e53/), [MobileNet](https://medium.com/towards-data-science/understanding-depthwise-separable-convolutions-and-the-efficiency-of-mobilenets-6de3d6b62503), [ShuffleNet](https://medium.com/towards-data-science/review-shufflenet-v1-light-weight-model-image-classification-5b253dfe982f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028a09b1-7807-4499-9560-3c15a93e9c0f",
   "metadata": {},
   "source": [
    "## Conclusions \n",
    "\n",
    "In this lecture, we explored the fundamentals of **PyTorch** and its application in deep learning, particularly in **image classification** using Convolutional Neural Networks (CNNs). We covered the key features of PyTorch, including its **dynamic computation graph and autograd system**, which facilitate efficient gradient computation for optimization. We also discussed **CNN architectures**, focusing on **convolutional operations, pooling, and hierarchical feature learning**, which allow networks to extract meaningful patterns from images. Additionally, we examined **AlexNet**, one of the pioneering deep learning models, and demonstrated how to preprocess and visualize the **CIFAR-10 dataset** for training and evaluation.  \n",
    "\n",
    "Through practical examples, we implemented a **complete image classification pipeline**, including **data loading, model training, and evaluation**. We also visualized model predictions to gain insights into its performance. Understanding how CNNs learn features at different depths and the role of the **receptive field** is crucial for designing more advanced architectures. Moving forward, students can explore **modern architectures like ResNet, transfer learning, and fine-tuning techniques** to improve classification accuracy on more complex datasets. Mastering these concepts provides a strong foundation for tackling real-world computer vision problems with deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357c4319-33e3-4d14-b818-6c49a21881f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
